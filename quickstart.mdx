---
title: "Quickstart"
description: "Get started with SigmaEval in minutes with this hands-on tutorial"
---

<Info>
  **Prerequisites**:
  - Python 3.10+
  - An API key for an LLM provider (e.g., OpenAI, Anthropic, Google)
</Info>

This guide will walk you through installing SigmaEval, setting up your first evaluation, and running a "Hello World" example.

## Installation

First, install the SigmaEval framework from PyPI. We recommend creating a virtual environment to avoid dependency conflicts.

```bash
pip install sigmaeval-framework
```

You will also need to set your API key for the LLM provider you wish to use for the AI Judge. SigmaEval supports 100+ LLM providers via [LiteLLM](https://litellm.ai/), including OpenAI, Anthropic, Google, and local models via Ollama.

<CodeGroup>
```bash OpenAI
export OPENAI_API_KEY="your-api-key"
```
```bash Anthropic
export ANTHROPIC_API_KEY="your-api-key"
```
```bash Gemini
export GEMINI_API_KEY="your-api-key"
```
```bash Ollama
# No API key needed for local Ollama models
# Set the model name directly, e.g., "ollama/llama3"
```
</CodeGroup>

## Hello World Example

Here is a minimal, complete example of how to use SigmaEval to test a simple AI application. This example evaluates a bot that is expected to list its capabilities when a user asks "what can you do?".

### 1. Define Your Application Logic

First, create a simple, stateless `app_handler` function. This function is what SigmaEval will call to interact with your application. It takes a list of messages and a state object and should return your bot's response as a string.

```python app.py
from typing import List, Dict, Any

async def app_handler(messages: List[Dict[str, str]], state: Any) -> str:
    """
    A stateless app_handler that returns a static response.
    """
    return "I can track orders, initiate returns, and answer product questions."
```

### 2. Define Your Evaluation Scenario

Next, define the test case using `ScenarioTest`. You'll describe the user's goal, the action they take, and what you expect the bot to do. You'll also specify the statistical criteria for success.

In this example, we expect that in at least 90% of conversations, the bot's response will earn a quality score of 7/10 or higher.

```python test_app.py
from sigmaeval import SigmaEval, ScenarioTest, assertions

# Define the test scenario
scenario = (
    ScenarioTest("Bot explains its capabilities")
    .given("A new user asks about what the bot can do")
    .when("The user asks 'what can you do?'")
    .expect_behavior(
        "Bot lists its main functions.",
        criteria=assertions.scores.proportion_gte(min_score=7, proportion=0.90)
    )
)
```

### 3. Run the Evaluation

Finally, create an instance of the `SigmaEval` class and call the `.evaluate()` method, passing in your scenario and `app_handler`. The `evaluate` method is an async function, so you'll need to run it within an async context.

```python test_app.py
import asyncio
from typing import List, Dict, Any

from sigmaeval import SigmaEval, ScenarioTest, assertions

# Your app_handler from Step 1
async def app_handler(messages: List[Dict[str, str]], state: Any) -> str:
    return "I can track orders, initiate returns, and answer product questions."

# Your scenario from Step 2
scenario = (
    ScenarioTest("Bot explains its capabilities")
    .given("A new user asks about what the bot can do")
    .when("The user asks 'what can you do?'")
    .expect_behavior(
        "Bot lists its main functions.",
        criteria=assertions.scores.proportion_gte(min_score=7, proportion=0.90)
    )
)

async def main():
    # Initialize SigmaEval
    # You can optionally set a global sample_size and significance_level
    sigma_eval = SigmaEval(
        judge_model="gemini/gemini-2.5-flash",
        sample_size=20,
        significance_level=0.05
    )

    # Run the evaluation
    result = await sigma_eval.evaluate(scenario, app_handler)

    # Print the detailed summary to the console
    print(result)

    # Programmatically check the result
    if result.passed:
        print("✅ Scenario passed!")
    else:
        print("❌ Scenario failed.")

if __name__ == "__main__":
    asyncio.run(main())
```

### 4. Interpret the Results

When you run the script, SigmaEval will simulate 20 conversations, have an AI Judge score each one, and then print a detailed summary of the results. The summary will show you the pass/fail status, the statistical outcomes, and the reasoning behind the scores.

Here's an example of what the output might look like:

```text
======================================================================
- SCENARIO: Bot explains its capabilities
======================================================================
  - GIVEN: A new user asks about what the bot can do
  - WHEN: The user asks 'what can you do?'

----------------------------------------------------------------------
- EXPECTATION: Bot lists its main functions.
----------------------------------------------------------------------
  - SCORES:
    - Median: 8.0
    - Mean: 8.2
    - Std Dev: 0.9
    - Histogram:
      - 6: ▇ (2)
      - 7: ▇▇▇ (6)
      - 8: ▇▇▇▇▇ (10)
      - 9: ▇ (2)
      - 10: ▇ (0)

  - CRITERIA:
    - ✅ proportion_gte(min_score=7, proportion=0.9)
      - Observed: 18/20 (90.0%) of scores were >= 7
      - p-value: 0.04 (Significant)

----------------------------------------------------------------------
- RESULT: ✅ PASSED
----------------------------------------------------------------------
```

This output confirms that the test passed because the proportion of conversations with a score of 7 or higher was sufficient to meet our statistical bar.

## Next Steps

Now that you've run your first evaluation, you can start applying SigmaEval to your own Gen AI applications.

<CardGroup cols={2}>
  <Card
    title="Core Concepts"
    icon="book-open"
    href="/development"
  >
    Learn about the core concepts behind SigmaEval's statistical approach.
  </Card>
  <Card
    title="API Reference"
    icon="code"
    href="/api-reference/introduction"
  >
    Explore the full API for advanced testing scenarios.
  </Card>
</CardGroup>
